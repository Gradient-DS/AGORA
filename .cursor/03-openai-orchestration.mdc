---
description: OpenAI-native orchestration with maximum leverage of OpenAI platform features
globs: server-openai/**/*
alwaysApply: false
---

# OpenAI-Native Orchestration Architecture

**Philosophy**: Maximize leverage of OpenAI's native features. Only build what's unique to AGORA's domain.

## Core Principles

1. **Use OpenAI Assistants API** - Native stateful agents with automatic tool loops
2. **Use Structured Outputs** - Guaranteed schema compliance, intelligent routing
3. **Use Parallel Function Calling** - 3-5x faster tool execution
4. **Use Built-in Tools** - Code interpreter, file search, vision
5. **Minimal Custom State** - Threads handle conversation state
6. **Minimal Custom Logic** - OpenAI handles tool loops, context management

## What We Build vs. What OpenAI Provides

### ❌ DON'T Build (OpenAI Provides This)

- ❌ Custom conversation state management → Use OpenAI Threads
- ❌ Manual token counting/truncation → Threads handle it
- ❌ Manual tool execution loops → Assistants API does it automatically
- ❌ Sequential tool execution → Use parallel_tool_calls=True
- ❌ Keyword-based routing → Use structured outputs
- ❌ Manual JSON parsing → Use structured outputs
- ❌ Custom checkpointing → Thread state is persistent
- ❌ Vector search implementation → Use file_search tool
- ❌ Code execution → Use code_interpreter tool

### ✅ DO Build (AGORA-Specific)

- ✅ HAI protocol implementation (WebSocket)
- ✅ MCP tool discovery and execution
- ✅ Domain-specific moderation/validation
- ✅ Business logic for agents
- ✅ Audit logging and observability
- ✅ Human-in-the-loop approval workflows
- ✅ Multi-implementation interface (Protocol layer)

## Project Structure

```
server-openai/
├── src/
│   └── agora_openai/
│       ├── config.py              # Pydantic settings
│       ├── logging_config.py      # Logging
│       ├── core/                  # Domain logic ONLY
│       │   ├── agent_definitions.py   # Agent configs for OpenAI
│       │   ├── routing_logic.py       # Structured output schemas
│       │   └── approval_logic.py      # Human-in-loop rules
│       ├── adapters/              # External integrations
│       │   ├── openai_assistants.py   # OpenAI Assistants API wrapper
│       │   ├── mcp_client.py          # MCP protocol client
│       │   └── audit_logger.py        # OpenTelemetry
│       ├── pipelines/             # Orchestration
│       │   ├── orchestrator.py        # Main flow coordinator
│       │   └── moderator.py           # Safety validation
│       └── api/                   # Entry point
│           ├── server.py              # FastAPI + WebSocket
│           └── hai_protocol.py        # HAI communication
├── common/                        # Shared interface
│   ├── hai_types.py              # HAI message types
│   ├── protocols.py              # Interface contracts
│   └── schemas.py                # Pydantic models
├── tests/
├── pyproject.toml
└── README.md
```

**Note**: No `state_manager.py`, no `memory_manager.py`, no custom DB schema for conversations!

## OpenAI Assistants Integration

### Agent Definitions (`core/agent_definitions.py`)

Define agents as configurations for OpenAI Assistants API.

```python
from typing import TypedDict

class AgentConfig(TypedDict):
    """Configuration for an OpenAI Assistant."""
    id: str
    name: str
    instructions: str
    model: str
    tools: list[str]  # Built-in tool types
    temperature: float

AGENT_CONFIGS: list[AgentConfig] = [
    {
        "id": "regulation-agent",
        "name": "Regulation Analysis Expert",
        "instructions": (
            "You are a regulatory compliance expert specializing in food safety, "
            "import/export regulations, and industry standards.\n\n"
            "YOUR CAPABILITIES:\n"
            "- Search and analyze regulatory documents using file_search\n"
            "- Execute compliance checks via MCP tools\n"
            "- Generate reports with code_interpreter for visualizations\n\n"
            "ALWAYS:\n"
            "- Cite specific regulations and standards\n"
            "- Provide actionable compliance guidance\n"
            "- Flag high-risk areas clearly\n"
            "- Use tools to verify current regulations\n\n"
            "FORMAT:\n"
            "Structure responses with: Summary, Details, Recommendations, Citations"
        ),
        "model": "gpt-4o",
        "tools": ["file_search", "code_interpreter"],  # Built-in tools
        "temperature": 0.3,
    },
    {
        "id": "risk-agent",
        "name": "Risk Assessment Specialist",
        "instructions": (
            "You are a risk management expert specializing in compliance risk, "
            "operational risk, and threat assessment.\n\n"
            "YOUR CAPABILITIES:\n"
            "- Assess risk levels using MCP risk tools\n"
            "- Analyze threat patterns with code_interpreter\n"
            "- Search historical incidents with file_search\n\n"
            "ALWAYS:\n"
            "- Provide risk severity ratings (Critical/High/Medium/Low)\n"
            "- Include likelihood and impact analysis\n"
            "- Recommend specific mitigation actions\n"
            "- Use parallel tool calls for comprehensive assessment\n\n"
            "FORMAT:\n"
            "Risk Summary → Analysis → Mitigation Plan → Monitoring Recommendations"
        ),
        "model": "gpt-4o",
        "tools": ["file_search", "code_interpreter"],
        "temperature": 0.2,
    },
    {
        "id": "reporting-agent",
        "name": "Report Generation Specialist",
        "instructions": (
            "You are a reporting and analytics expert creating comprehensive "
            "compliance and operational reports.\n\n"
            "YOUR CAPABILITIES:\n"
            "- Generate charts and visualizations with code_interpreter\n"
            "- Aggregate data from multiple MCP sources\n"
            "- Search documents for supporting evidence with file_search\n"
            "- Create executive summaries\n\n"
            "ALWAYS:\n"
            "- Use code_interpreter for data visualization\n"
            "- Include key metrics and trends\n"
            "- Provide executive summary at top\n"
            "- Cite data sources\n\n"
            "FORMAT:\n"
            "Executive Summary → Key Findings → Detailed Analysis → Recommendations"
        ),
        "model": "gpt-4o",
        "tools": ["file_search", "code_interpreter"],
        "temperature": 0.4,
    },
]
```

**Key Points:**
- Define instructions, not implementations
- Specify OpenAI's built-in tools (file_search, code_interpreter)
- MCP tools added dynamically at runtime
- Temperature tuned per agent role

### Routing Logic (`core/routing_logic.py`)

Use structured outputs for intelligent agent selection.

```python
from pydantic import BaseModel, Field
from typing import Literal

class AgentSelection(BaseModel):
    """Structured output for intelligent agent routing."""
    selected_agent: Literal[
        "regulation-agent",
        "risk-agent",
        "reporting-agent",
    ] = Field(description="The most appropriate agent for this request")
    
    reasoning: str = Field(
        description="Brief explanation of why this agent was selected"
    )
    
    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence score for this selection"
    )
    
    requires_multiple_agents: bool = Field(
        default=False,
        description="Whether this request requires multiple specialized agents"
    )
    
    suggested_follow_up_agents: list[str] = Field(
        default_factory=list,
        description="Additional agents to consult if needed"
    )

ROUTING_SYSTEM_PROMPT = """You are an intelligent routing system for a compliance platform.

Analyze the user's request and select the most appropriate specialized agent:

**regulation-agent**: 
- Regulatory compliance questions
- Legal requirements
- Standards and certifications
- Import/export regulations
- Industry-specific rules

**risk-agent**:
- Risk assessment and analysis
- Threat identification
- Vulnerability assessment
- Security concerns
- Incident analysis

**reporting-agent**:
- Report generation
- Data analysis and visualization
- Compliance summaries
- Trend analysis
- Performance metrics

Consider:
1. Primary topic and domain
2. Required expertise level
3. Whether multiple agents might be needed
4. User's apparent intent

Return your selection with reasoning and confidence score."""
```

**Key Points:**
- Pydantic model ensures type safety
- Structured outputs guarantee schema compliance
- Includes confidence scoring
- Supports multi-agent workflows

### Approval Logic (`core/approval_logic.py`)

Define rules for human-in-the-loop.

```python
from typing import Any
from common.schemas import ToolCall

HIGH_RISK_TOOL_PATTERNS = [
    "delete",
    "remove",
    "destroy",
    "drop",
    "submit_final",
    "approve_compliance",
    "publish_report",
    "certify",
]

HIGH_RISK_PARAMETERS = {
    "amount": 10000,  # Threshold for financial operations
    "scope": ["company_wide", "global"],
}

def requires_human_approval(
    tool_calls: list[ToolCall],
    context: dict[str, Any],
) -> tuple[bool, str | None]:
    """Determine if tool execution requires human approval.
    
    Pure business logic - no I/O.
    """
    for tool_call in tool_calls:
        tool_name_lower = tool_call.tool_name.lower()
        
        for pattern in HIGH_RISK_TOOL_PATTERNS:
            if pattern in tool_name_lower:
                return True, f"High-risk operation detected: {tool_call.tool_name}"
        
        params = tool_call.parameters or {}
        if "amount" in params and params["amount"] > HIGH_RISK_PARAMETERS["amount"]:
            return True, f"Amount exceeds threshold: {params['amount']}"
        
        if "scope" in params and params["scope"] in HIGH_RISK_PARAMETERS["scope"]:
            return True, f"Company-wide scope requires approval"
    
    return False, None
```

## OpenAI Assistants Client (`adapters/openai_assistants.py`)

Thin wrapper around OpenAI Assistants API.

```python
from __future__ import annotations
from typing import Any, Callable
import logging
import asyncio
from openai import AsyncOpenAI

log = logging.getLogger(__name__)

class OpenAIAssistantsClient:
    """Wrapper for OpenAI Assistants API - leverages native features."""
    
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)
        self.assistants: dict[str, str] = {}  # agent_id -> assistant_id
    
    async def initialize_assistant(
        self,
        agent_id: str,
        name: str,
        instructions: str,
        model: str,
        tools: list[dict[str, Any]],
        temperature: float = 0.7,
    ) -> str:
        """Create OpenAI Assistant. Returns assistant_id."""
        assistant = await self.client.beta.assistants.create(
            name=name,
            instructions=instructions,
            model=model,
            tools=tools,
            temperature=temperature,
        )
        
        self.assistants[agent_id] = assistant.id
        log.info("Created assistant %s: %s", agent_id, assistant.id)
        return assistant.id
    
    async def create_thread(self, metadata: dict[str, Any] | None = None) -> str:
        """Create conversation thread. Returns thread_id."""
        thread = await self.client.beta.threads.create(metadata=metadata or {})
        log.info("Created thread: %s", thread.id)
        return thread.id
    
    async def send_message(self, thread_id: str, content: str) -> None:
        """Add user message to thread."""
        await self.client.beta.threads.messages.create(
            thread_id=thread_id,
            role="user",
            content=content,
        )
    
    async def run_assistant_with_tools(
        self,
        thread_id: str,
        assistant_id: str,
        tool_executor: Callable[[str, dict], Any],
    ) -> str:
        """Run assistant with automatic tool execution loop.
        
        OpenAI handles the loop automatically!
        """
        run = await self.client.beta.threads.runs.create(
            thread_id=thread_id,
            assistant_id=assistant_id,
        )
        
        while True:
            await asyncio.sleep(0.5)
            
            run = await self.client.beta.threads.runs.retrieve(
                thread_id=thread_id,
                run_id=run.id,
            )
            
            log.info("Run %s status: %s", run.id, run.status)
            
            if run.status == "completed":
                break
            
            elif run.status == "requires_action":
                # Execute tools and submit results
                tool_outputs = await self._execute_required_tools(
                    run,
                    tool_executor,
                )
                
                await self.client.beta.threads.runs.submit_tool_outputs(
                    thread_id=thread_id,
                    run_id=run.id,
                    tool_outputs=tool_outputs,
                )
            
            elif run.status in ["failed", "cancelled", "expired"]:
                raise Exception(f"Run {run.status}: {run.last_error}")
        
        # Get final response
        messages = await self.client.beta.threads.messages.list(
            thread_id=thread_id,
            order="desc",
            limit=1,
        )
        
        return messages.data[0].content[0].text.value
    
    async def _execute_required_tools(
        self,
        run: Any,
        tool_executor: Callable,
    ) -> list[dict[str, str]]:
        """Execute tools that OpenAI requests."""
        import json
        
        tool_outputs = []
        
        for tool_call in run.required_action.submit_tool_outputs.tool_calls:
            try:
                args = json.loads(tool_call.function.arguments)
                result = await tool_executor(tool_call.function.name, args)
                
                tool_outputs.append({
                    "tool_call_id": tool_call.id,
                    "output": json.dumps(result),
                })
            except Exception as e:
                log.error("Tool execution failed: %s", e)
                tool_outputs.append({
                    "tool_call_id": tool_call.id,
                    "output": json.dumps({"error": str(e)}),
                })
        
        return tool_outputs
    
    async def route_with_structured_output(
        self,
        message: str,
        context: dict[str, Any],
        response_model: type,
    ) -> Any:
        """Use structured outputs for intelligent routing.
        
        Guaranteed schema compliance!
        """
        from core.routing_logic import ROUTING_SYSTEM_PROMPT
        
        response = await self.client.beta.chat.completions.parse(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": ROUTING_SYSTEM_PROMPT},
                {"role": "user", "content": f"Request: {message}\n\nContext: {context}"},
            ],
            response_format=response_model,
        )
        
        return response.choices[0].message.parsed
```

**Key Points:**
- OpenAI handles tool execution loop
- Structured outputs for routing
- No manual state management
- No token counting
- No context truncation

## Orchestration Pipeline (`pipelines/orchestrator.py`)

Minimal orchestration - OpenAI does the heavy lifting.

```python
from __future__ import annotations
import logging
from common.hai_types import UserMessage, AssistantMessage
from core.routing_logic import AgentSelection
from core.approval_logic import requires_human_approval
from adapters.openai_assistants import OpenAIAssistantsClient
from adapters.mcp_client import MCPToolClient
from pipelines.moderator import ModerationPipeline

log = logging.getLogger(__name__)

class Orchestrator:
    """Minimal orchestration - OpenAI handles complexity."""
    
    def __init__(
        self,
        openai_client: OpenAIAssistantsClient,
        mcp_client: MCPToolClient,
        moderator: ModerationPipeline,
    ):
        self.openai = openai_client
        self.mcp = mcp_client
        self.moderator = moderator
        self.threads: dict[str, str] = {}  # session_id -> thread_id
    
    async def process_message(
        self,
        message: UserMessage,
        session_id: str,
    ) -> AssistantMessage:
        """Process message with OpenAI-native features."""
        
        # 1. Validate input
        is_valid, error = await self.moderator.validate_input(message.content)
        if not is_valid:
            return AssistantMessage(content=f"Input validation failed: {error}")
        
        # 2. Get or create thread (OpenAI handles state!)
        if session_id not in self.threads:
            self.threads[session_id] = await self.openai.create_thread({
                "session_id": session_id,
            })
        thread_id = self.threads[session_id]
        
        # 3. Intelligent routing with structured outputs
        routing = await self.openai.route_with_structured_output(
            message=message.content,
            context={"session_id": session_id},
            response_model=AgentSelection,
        )
        
        log.info(
            "Routed to %s (confidence: %.2f): %s",
            routing.selected_agent,
            routing.confidence,
            routing.reasoning,
        )
        
        # 4. Send message to thread
        await self.openai.send_message(thread_id, message.content)
        
        # 5. Run assistant (automatic tool loop!)
        assistant_id = self.openai.assistants[routing.selected_agent]
        
        response_content = await self.openai.run_assistant_with_tools(
            thread_id=thread_id,
            assistant_id=assistant_id,
            tool_executor=self.mcp.execute_tool,
        )
        
        # 6. Validate output
        is_valid, error = await self.moderator.validate_output(response_content)
        if not is_valid:
            log.warning("Output validation failed: %s", error)
            response_content = "I apologize, but I cannot provide that response."
        
        # 7. Return response
        return AssistantMessage(content=response_content)
```

**Total Lines**: ~80 vs. previous ~300+

**What OpenAI Handles:**
- Conversation state (threads)
- Tool execution loop
- Context management
- Token counting
- Memory retention

**What We Handle:**
- Routing (with OpenAI structured outputs)
- Validation
- MCP tool integration
- Session mapping

## MCP Integration (`adapters/mcp_client.py`)

MCP tools are injected into OpenAI Assistants.

```python
from __future__ import annotations
from typing import Any
import logging

log = logging.getLogger(__name__)

class MCPToolClient:
    """MCP protocol client for tool discovery and execution."""
    
    def __init__(self, server_urls: dict[str, str]):
        self.servers = server_urls
        self.tool_definitions: list[dict[str, Any]] = []
    
    async def discover_tools(self) -> list[dict[str, Any]]:
        """Discover tools from MCP servers.
        
        Returns tools in OpenAI function format.
        """
        tools = []
        
        for server_name, url in self.servers.items():
            server_tools = await self._discover_from_server(server_name, url)
            tools.extend(server_tools)
        
        self.tool_definitions = tools
        log.info("Discovered %d tools from %d servers", len(tools), len(self.servers))
        return tools
    
    async def _discover_from_server(
        self,
        server_name: str,
        url: str,
    ) -> list[dict[str, Any]]:
        """Discover tools from single MCP server."""
        # MCP protocol implementation
        # Return tools in OpenAI format
        return []
    
    async def execute_tool(
        self,
        tool_name: str,
        parameters: dict[str, Any],
    ) -> dict[str, Any]:
        """Execute tool via MCP protocol.
        
        Called by OpenAI during automatic tool loop.
        """
        log.info("Executing MCP tool: %s", tool_name)
        
        # Execute via MCP protocol
        result = {"status": "success", "data": {}}
        
        return result
```

## Built-in Tools Configuration

OpenAI provides these tools natively - we just enable them!

```python
def get_builtin_tools() -> list[dict[str, Any]]:
    """Get OpenAI's built-in tools to enable."""
    return [
        {
            "type": "code_interpreter",
            # Enables:
            # - Python code execution in sandbox
            # - Chart/visualization generation
            # - Data analysis
            # - Mathematical computations
        },
        {
            "type": "file_search",
            # Enables:
            # - Vector search over uploaded files
            # - Automatic embeddings
            # - RAG capabilities
            # - Document analysis
        },
    ]
```

**Use Cases:**
- **code_interpreter**: Generate compliance report charts, analyze data trends
- **file_search**: Search regulations, find relevant policy documents

## Initialization (`api/server.py`)

Startup initializes OpenAI Assistants.

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from agora_openai.config import get_settings
from agora_openai.core.agent_definitions import AGENT_CONFIGS
from agora_openai.adapters.openai_assistants import OpenAIAssistantsClient
from agora_openai.adapters.mcp_client import MCPToolClient

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize OpenAI Assistants on startup."""
    settings = get_settings()
    
    # Initialize clients
    openai_client = OpenAIAssistantsClient(
        api_key=settings.openai_api_key.get_secret_value()
    )
    
    mcp_client = MCPToolClient(parse_mcp_servers(settings.mcp_servers))
    
    # Discover MCP tools
    mcp_tools = await mcp_client.discover_tools()
    
    # Create OpenAI Assistants with all tools
    for agent_config in AGENT_CONFIGS:
        # Combine built-in tools + MCP tools
        all_tools = [
            {"type": tool_type}
            for tool_type in agent_config["tools"]
        ] + mcp_tools
        
        await openai_client.initialize_assistant(
            agent_id=agent_config["id"],
            name=agent_config["name"],
            instructions=agent_config["instructions"],
            model=agent_config["model"],
            tools=all_tools,
            temperature=agent_config["temperature"],
        )
    
    app.state.openai = openai_client
    app.state.mcp = mcp_client
    
    yield
    
    # Cleanup (OpenAI handles assistant lifecycle)

app = FastAPI(lifespan=lifespan)
```

## Configuration (`config.py`)

```python
from pydantic_settings import BaseSettings
from pydantic import SecretStr

class Settings(BaseSettings):
    """Application settings."""
    
    # OpenAI
    openai_api_key: SecretStr
    openai_model: str = "gpt-4o"
    
    # MCP Servers
    mcp_servers: str  # server1=url1,server2=url2
    
    # Moderation
    guardrails_enabled: bool = True
    
    # Observability
    otel_endpoint: str = "http://localhost:4317"
    
    # NOTE: No database_url! OpenAI Threads handle state.
    
    class Config:
        env_file = ".env"
        env_prefix = "APP_"
```

**Key Point**: No PostgreSQL needed for conversation state! OpenAI Threads persist automatically.

## Testing

Test with mocks for OpenAI Assistants API.

```python
# tests/conftest.py

class MockOpenAIAssistants:
    """Mock OpenAI Assistants API."""
    
    def __init__(self):
        self.threads: dict[str, list[str]] = {}
        self.assistants: dict[str, Any] = {}
    
    async def create_thread(self, metadata=None):
        thread_id = f"thread_{len(self.threads)}"
        self.threads[thread_id] = []
        return type('Thread', (), {'id': thread_id})()
    
    async def run_assistant_with_tools(self, thread_id, assistant_id, tool_executor):
        return "Mock response from assistant"
```

## Performance Characteristics

### With OpenAI-Native Approach

**Message Processing:**
- Routing: ~200ms (structured output)
- Tool execution: ~1-2s (parallel calls)
- Response: ~500ms
- **Total: ~2s**

**Parallel Tool Calling:**
- 3 tools @ 2s each: **2s total** (not 6s!)

**Code Reduction:**
- State management: ~~300 lines~~ → 0 lines
- Memory management: ~~150 lines~~ → 0 lines  
- Tool loops: ~~100 lines~~ → 0 lines
- Agent selection: ~~50 lines~~ → 10 lines (schema only)
- **Total: ~800 lines → ~200 lines**

## Migration from Custom Implementation

If migrating from custom implementation:

### Phase 1: Enable Parallel Calling
```python
# Before
for tool in tools:
    await execute(tool)  # Sequential

# After  
parallel_tool_calls=True  # OpenAI handles it!
```

### Phase 2: Use Structured Outputs for Routing
```python
# Before
if "regulation" in message.lower():
    agent = "regulation-agent"

# After
routing = await openai.route_with_structured_output(
    message, context, AgentSelection
)
agent = routing.selected_agent
```

### Phase 3: Migrate to Assistants API
```python
# Before
state = await db.load_state(session_id)
messages = truncate_to_token_limit(state.messages)
response = await openai.chat.completions.create(messages=messages)
await db.save_state(session_id, new_state)

# After
thread_id = await openai.create_thread()  # Once per session
response = await openai.run_assistant(thread_id, assistant_id)
# State handled automatically by OpenAI!
```

### Phase 4: Enable Built-in Tools
```python
tools = [
    {"type": "code_interpreter"},  # Free Python sandbox!
    {"type": "file_search"},       # Free vector search!
] + mcp_tools
```

## Best Practices

### 1. Let OpenAI Handle State
```python
# ❌ DON'T
await db.save_conversation_state(session_id, messages)

# ✅ DO  
thread_id = await openai.create_thread()
# OpenAI persists everything automatically
```

### 2. Use Structured Outputs
```python
# ❌ DON'T
response = await openai.chat.completions.create(...)
data = json.loads(response.content)  # Might fail!

# ✅ DO
response = await openai.chat.completions.parse(
    ...,
    response_format=MySchema,
)
data = response.choices[0].message.parsed  # Guaranteed valid!
```

### 3. Enable Parallel Tools
```python
# ❌ DON'T
for tool in tools:
    await execute_tool(tool)

# ✅ DO
response = await openai.chat.completions.create(
    ...,
    parallel_tool_calls=True,
)
```

### 4. Leverage Built-in Tools
```python
# ❌ DON'T build your own
custom_code_executor()
custom_vector_search()

# ✅ DO use OpenAI's
tools=[
    {"type": "code_interpreter"},
    {"type": "file_search"},
]
```

### 5. Instructions Over Code
```python
# ❌ DON'T write logic in code
if agent == "risk":
    prompt = "Assess risk..."
    format_check(response)

# ✅ DO write logic in instructions
instructions = """
You are a risk expert.
ALWAYS include severity rating.
ALWAYS format as: Summary → Analysis → Recommendations
"""
```

## Anti-Patterns to Avoid

❌ **Custom State Management**
```python
# NO!
class StateManager:
    async def save_state(self, session_id, state):
        await self.db.save(...)
```

❌ **Manual Token Counting**
```python
# NO!
tokens = count_tokens(messages)
if tokens > limit:
    messages = truncate(messages)
```

❌ **Manual Tool Loops**
```python
# NO!
while has_tool_calls:
    results = execute_tools()
    response = call_model_again(results)
```

❌ **Keyword Routing**
```python
# NO!
if "regulation" in message:
    agent = "regulation-agent"
```

❌ **Sequential Tool Execution**
```python
# NO!
for tool in tool_calls:
    await execute(tool)
```

## Summary

**Use OpenAI For:**
- ✅ Conversation state (Threads)
- ✅ Tool execution loops (Assistants)
- ✅ Parallel tool calling
- ✅ Agent routing (Structured outputs)
- ✅ Code execution (code_interpreter)
- ✅ Vector search (file_search)
- ✅ Context management
- ✅ Token counting

**Build Only:**
- ✅ Agent instructions (domain expertise)
- ✅ MCP tool integration
- ✅ HAI protocol
- ✅ Moderation/validation
- ✅ Approval workflows
- ✅ Audit logging

**Result:**
- 75% less code
- 2-3x faster
- More reliable
- Better results
- Future-proof (OpenAI improvements benefit you automatically)
